{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "model_checkpoint = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    model_checkpoint,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")\n",
    "\n",
    "jobs = [\n",
    "    \"Lập trình viên Python tại công ty ABC.\",\n",
    "    \"Chuyên viên phân tích dữ liệu tại công ty XYZ.\",\n",
    "    \"Nhân viên kinh doanh phần mềm cho công ty DEF.\",\n",
    "]\n",
    "\n",
    "cvs = [\n",
    "    \"Tôi có 3 năm kinh nghiệm phát triển Python và sử dụng Flask.\",\n",
    "    \"Kỹ năng phân tích dữ liệu với Python, R, và SQL, 2 năm kinh nghiệm.\",\n",
    "    \"Tôi từng làm nhân viên kinh doanh phần mềm trong 4 năm.\",\n",
    "]\n",
    "\n",
    "job_embeddings = model.encode(jobs)\n",
    "cv_embeddings = model.encode(cvs)\n",
    "\n",
    "# Tính cosine similarity giữa tất cả các cặp (CV, Job)\n",
    "similarities = model.similarity(cv_embeddings, job_embeddings)\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dataset = load_dataset(\"HZeroxium/job-cv-binary\")\n",
    "triplet_dataset = load_dataset(\"HZeroxium/cv-job-triplet\")\n",
    "similarity_dataset = load_dataset(\"HZeroxium/cv-job-similarity\")\n",
    "job_paraphrase_dataset = load_dataset(\"HZeroxium/job-paraphrase\")\n",
    "cv_paraphrase_dataset = load_dataset(\"HZeroxium/cv-paraphrase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_paraphrase_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_paraphrase_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_binary_dataset = binary_dataset[\"train\"]\n",
    "eval_binary_dataset = binary_dataset[\"test\"]\n",
    "\n",
    "train_triplet_dataset = triplet_dataset[\"train\"]\n",
    "eval_triplet_dataset = triplet_dataset[\"test\"]\n",
    "\n",
    "train_similarity_dataset = similarity_dataset[\"train\"]\n",
    "eval_similarity_dataset = similarity_dataset[\"test\"]\n",
    "\n",
    "train_job_paraphrase_dataset = job_paraphrase_dataset[\"train\"]\n",
    "eval_job_paraphrase_dataset = job_paraphrase_dataset[\"test\"]\n",
    "\n",
    "train_cv_paraphrase_dataset = cv_paraphrase_dataset[\"train\"]\n",
    "eval_cv_paraphrase_dataset = cv_paraphrase_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {\n",
    "    \"binary\": train_binary_dataset,\n",
    "    \"triplet\": train_triplet_dataset,\n",
    "    \"similarity\": train_similarity_dataset,\n",
    "    \"job_paraphrase\": train_job_paraphrase_dataset,\n",
    "    \"cv_paraphrase\": train_cv_paraphrase_dataset,\n",
    "}\n",
    "\n",
    "eval_dataset = {\n",
    "    \"binary\": eval_binary_dataset,\n",
    "    \"triplet\": eval_triplet_dataset,\n",
    "    \"similarity\": eval_similarity_dataset,\n",
    "    \"job_paraphrase\": eval_job_paraphrase_dataset,\n",
    "    \"cv_paraphrase\": eval_cv_paraphrase_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import (\n",
    "    ContrastiveLoss,\n",
    "    TripletLoss,\n",
    "    CoSENTLoss,\n",
    "    MultipleNegativesRankingLoss,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "binary_loss = ContrastiveLoss(model)\n",
    "triplet_loss = TripletLoss(model)\n",
    "\n",
    "\n",
    "\n",
    "similarity_loss = CoSENTLoss(model)\n",
    "job_paraphrase_loss = MultipleNegativesRankingLoss(model)\n",
    "cv_paraphrase_loss = MultipleNegativesRankingLoss(model)\n",
    "\n",
    "\n",
    "losses = {\n",
    "    \"binary\": binary_loss,\n",
    "    \"triplet\": triplet_loss,\n",
    "    \"similarity\": similarity_loss,\n",
    "    \"job_paraphrase\": job_paraphrase_loss,\n",
    "    \"cv_paraphrase\": cv_paraphrase_loss,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import (\n",
    "    BinaryClassificationEvaluator,\n",
    "    TripletEvaluator,\n",
    "    EmbeddingSimilarityEvaluator,\n",
    ")\n",
    "\n",
    "job_scores = [1] * len(eval_job_paraphrase_dataset[\"text1\"])\n",
    "cv_scores = [1] * len(eval_cv_paraphrase_dataset[\"text1\"])\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(\n",
    "    sentences1=eval_binary_dataset[\"text1\"],\n",
    "    sentences2=eval_binary_dataset[\"text2\"],\n",
    "    labels=eval_binary_dataset[\"label\"],\n",
    ")\n",
    "\n",
    "triplet_evaluator = TripletEvaluator(\n",
    "    anchors=eval_triplet_dataset[\"anchor\"],\n",
    "    positives=eval_triplet_dataset[\"positive\"],\n",
    "    negatives=eval_triplet_dataset[\"negative\"],\n",
    ")\n",
    "\n",
    "similarity_evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=eval_similarity_dataset[\"text1\"],\n",
    "    sentences2=eval_similarity_dataset[\"text2\"],\n",
    "    scores=eval_similarity_dataset[\"score\"],\n",
    ")\n",
    "\n",
    "job_paraphrase_evaluator = BinaryClassificationEvaluator(\n",
    "    sentences1=eval_job_paraphrase_dataset[\"text1\"],\n",
    "    sentences2=eval_job_paraphrase_dataset[\"text2\"],\n",
    "    labels=job_scores,\n",
    ")\n",
    "\n",
    "cv_paraphrase_evaluator = BinaryClassificationEvaluator(\n",
    "    sentences1=eval_cv_paraphrase_dataset[\"text1\"],\n",
    "    sentences2=eval_cv_paraphrase_dataset[\"text2\"],\n",
    "    labels=cv_scores,\n",
    ")\n",
    "\n",
    "\n",
    "from sentence_transformers.evaluation import SequentialEvaluator\n",
    "\n",
    "evaluator = SequentialEvaluator(\n",
    "    [\n",
    "        binary_evaluator,\n",
    "        triplet_evaluator,\n",
    "        similarity_evaluator,\n",
    "        job_paraphrase_evaluator,\n",
    "        cv_paraphrase_evaluator,\n",
    "    ]\n",
    ")\n",
    "evaluator(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import (\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments,\n",
    ")\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    # Required parameter:\n",
    "    output_dir=f\"./models/{model_checkpoint}-job-cv-multi-dataset\",\n",
    "    # Optional training parameters:\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_ratio=0.1,\n",
    "    fp16=True,  # Set to False if you get an error that your GPU can't run on FP16\n",
    "    bf16=False,  # Set to True if you have a GPU that supports BF16\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # losses that use \"in-batch negatives\" benefit from no duplicates\n",
    "    # Optional tracking/debugging parameters:\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=200,\n",
    "    run_name=\"triplet-job-cv-multi-dataset\",  # Will be used in W&B if `wandb` is installed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    loss=losses,\n",
    "    evaluator=evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers.trainer_callback import TrainerState\n",
    "import os\n",
    "import json\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "# Ghi đè hàm save_to_json để chuyển đổi numpy floats\n",
    "def custom_save_to_json(self, json_path: str):\n",
    "    def convert_to_native(obj):\n",
    "        if isinstance(obj, (np.float32, np.float64)):\n",
    "            return float(obj)\n",
    "        return obj\n",
    "\n",
    "    # Chuyển đổi tất cả các trường thành kiểu có thể lưu JSON\n",
    "    data = json.loads(json.dumps(dataclasses.asdict(self), default=convert_to_native))\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, sort_keys=True)\n",
    "\n",
    "\n",
    "# Thay thế phương thức trong TrainerState\n",
    "TrainerState.save_to_json = custom_save_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_embeddings = model.encode(jobs)\n",
    "cv_embeddings = model.encode(cvs)\n",
    "\n",
    "# Tính cosine similarity giữa tất cả các cặp (CV, Job)\n",
    "similarities = model.similarity(cv_embeddings, job_embeddings)\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(path=f\"./models/{model_checkpoint}-job-cv-multi-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Ghi đè phương thức read_text để đọc tất cả tệp dưới dạng UTF-8\n",
    "original_read_text = Path.read_text\n",
    "\n",
    "\n",
    "def read_text_utf8(path, *args, **kwargs):\n",
    "    kwargs[\"encoding\"] = \"utf-8\"\n",
    "    return original_read_text(path, *args, **kwargs)\n",
    "\n",
    "\n",
    "Path.read_text = read_text_utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
