{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Desktop\\OpenCV\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_uncased_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "bert_cased_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "phobert_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n",
    "\n",
    "sentence = \"Lập trình viên Python. Chuyên gia Machine Learning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Tokens by BERT uncased:\n",
      "['lap', 'tri', '##nh', 'vie', '##n', 'python', '.', 'chu', '##yen', 'gia', 'machine', 'learning', '.']\n",
      "==> Tokens by BERT cased:\n",
      "['L', '##ậ', '##p', 't', '##r', '##ì', '##nh', 'v', '##i', '##ê', '##n', 'Python', '.', 'Chu', '##y', '##ê', '##n', 'g', '##ia', 'Machine', 'Learning', '.']\n",
      "==> Tokens by RoBERTa:\n",
      "['▁Lập', '▁trình', '▁viên', '▁Python', '.', '▁Chuyên', '▁gia', '▁Machine', '▁Learning', '.']\n",
      "==> Tokens by PhoBERT:\n",
      "['Lập', 'trình', 'viên', 'Py@@', 'th@@', 'on.', 'Chuyên', 'gia', 'Mach@@', 'ine', 'Le@@', 'arn@@', 'ing@@', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"==> Tokens by BERT uncased:\")\n",
    "print(bert_uncased_tokenizer.tokenize(sentence))\n",
    "print(\"==> Tokens by BERT cased:\")\n",
    "print(bert_cased_tokenizer.tokenize(sentence))\n",
    "print(\"==> Tokens by RoBERTa:\")\n",
    "print(roberta_tokenizer.tokenize(sentence))\n",
    "print(\"==> Tokens by PhoBERT:\")\n",
    "print(phobert_tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded IDs (BERT uncased): [101, 5001, 13012, 25311, 20098, 2078, 18750, 1012, 14684, 20684, 27699, 3698, 4083, 1012, 102]\n",
      "Decoded sentence (BERT uncased): [CLS] lap trinh vien python. chuyen gia machine learning. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Encode and decode (BERT uncased)\n",
    "encoded_input = bert_uncased_tokenizer(sentence, return_tensors=\"pt\")\n",
    "decoded_input = bert_uncased_tokenizer.decode(encoded_input[\"input_ids\"][0])\n",
    "print(\"\\nEncoded IDs (BERT uncased):\", encoded_input[\"input_ids\"][0].tolist())\n",
    "print(\"Decoded sentence (BERT uncased):\", decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded IDs (BERT cased): [101, 149, 28645, 1643, 189, 1197, 21409, 15624, 191, 1182, 24559, 1179, 23334, 119, 17144, 1183, 24559, 1179, 176, 1465, 7792, 9681, 119, 102]\n",
      "Decoded sentence (BERT cased): [CLS] Lập trình viên Python. Chuyên gia Machine Learning. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Encode and decode (BERT cased)\n",
    "encoded_input = bert_cased_tokenizer(sentence, return_tensors=\"pt\")\n",
    "decoded_input = bert_cased_tokenizer.decode(encoded_input[\"input_ids\"][0])\n",
    "print(\"\\nEncoded IDs (BERT cased):\", encoded_input[\"input_ids\"][0].tolist())\n",
    "print(\"Decoded sentence (BERT cased):\", decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded IDs (PhoBERT): [0, 6081, 1893, 1430, 10085, 1981, 34412, 15806, 3931, 27341, 3403, 2923, 50941, 5936, 5, 2]\n",
      "Decoded sentence (PhoBERT): <s> Lập trình viên Python. Chuyên gia Machine Learning. </s>\n"
     ]
    }
   ],
   "source": [
    "# Encode and Decode (PhoBERT)\n",
    "encoded_input = phobert_tokenizer(sentence, return_tensors=\"pt\")\n",
    "decoded_input = phobert_tokenizer.decode(encoded_input[\"input_ids\"][0])\n",
    "print(\"\\nEncoded IDs (PhoBERT):\", encoded_input[\"input_ids\"][0].tolist())\n",
    "print(\"Decoded sentence (PhoBERT):\", decoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded IDs (RoBERTa): [0, 189041, 5009, 4603, 145581, 5, 115818, 3529, 68311, 114344, 5, 2]\n",
      "Decoded sentence (RoBERTa): <s> Lập trình viên Python. Chuyên gia Machine Learning.</s>\n"
     ]
    }
   ],
   "source": [
    "# Encode and Decode (RoBERTa)\n",
    "encoded_input = roberta_tokenizer(sentence, return_tensors=\"pt\")\n",
    "decoded_input = roberta_tokenizer.decode(encoded_input[\"input_ids\"][0])\n",
    "print(\"\\nEncoded IDs (RoBERTa):\", encoded_input[\"input_ids\"][0].tolist())\n",
    "print(\"Decoded sentence (RoBERTa):\", decoded_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
